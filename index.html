<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
   <meta http-equiv="content-type" content="text/html;charset=UTF-8">
   <style type="text/css">
   /*CSS stylesheet is based on killwing's flavored markdown style:https://gist.github.com/2937864*/body{    margin: 0 auto;    font: 13px/1.231 Helvetica, Arial, sans-serif;    color: #444444;    line-height: 1;    max-width: 960px;    padding: 5px;}h1, h2, h3, h4 {    color: #111111;    font-weight: 400;}h1, h2, h3, h4, h5, p {    margin-bottom: 16px;    padding: 0;}h1 {    font-size: 28px;}h2 {    font-size: 22px;    margin: 20px 0 6px;}h3 {    font-size: 21px;}h4 {    font-size: 18px;}h5 {    font-size: 16px;}a {    color: #0099ff;    margin: 0;    padding: 0;    vertical-align: baseline;}a:link,a:visited{ text-decoration:none;}a:hover{ text-decoration:underline;}ul, ol {    padding: 0;    margin: 0;}li {    line-height: 24px;    margin-left: 44px;}li ul, li ul {    margin-left: 24px;}ul, ol {    font-size: 14px;    line-height: 20px;    max-width: 540px;}p {    font-size: 14px;    line-height: 20px;    max-width: 540px;    margin-top: 3px;}pre {    padding: 0px 4px;    max-width: 800px;    white-space: pre-wrap;    font-family: Consolas, Monaco, Andale Mono, monospace;    line-height: 1.5;    font-size: 13px;    border: 1px solid #ddd;    background-color: #f7f7f7;    border-radius: 3px;}code {    font-family: Consolas, Monaco, Andale Mono, monospace;    line-height: 1.5;    font-size: 13px;    border: 1px solid #ddd;    background-color: #f7f7f7;    border-radius: 3px;}pre code {    border: 0px;}aside {    display: block;    float: right;    width: 390px;}blockquote {    border-left:.5em solid #40AA53;    padding: 0 2em;    margin-left:0;    max-width: 476px;}blockquote  cite {    font-size:14px;    line-height:20px;    color:#bfbfbf;}blockquote cite:before {    content: '\2014 \00A0';}blockquote p {      color: #666;    max-width: 460px;}hr {    height: 1px;    border: none;    border-top: 1px dashed #0066CC}button,input,select,textarea {  font-size: 100%;  margin: 0;  vertical-align: baseline;  *vertical-align: middle;}button, input {  line-height: normal;  *overflow: visible;}button::-moz-focus-inner, input::-moz-focus-inner {  border: 0;  padding: 0;}button,input[type="button"],input[type="reset"],input[type="submit"] {  cursor: pointer;  -webkit-appearance: button;}input[type=checkbox], input[type=radio] {  cursor: pointer;}/* override default chrome & firefox settings */input:not([type="image"]), textarea {  -webkit-box-sizing: content-box;  -moz-box-sizing: content-box;  box-sizing: content-box;}input[type="search"] {  -webkit-appearance: textfield;  -webkit-box-sizing: content-box;  -moz-box-sizing: content-box;  box-sizing: content-box;}input[type="search"]::-webkit-search-decoration {  -webkit-appearance: none;}label,input,select,textarea {  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;  font-size: 13px;  font-weight: normal;  line-height: normal;  margin-bottom: 18px;}input[type=checkbox], input[type=radio] {  cursor: pointer;  margin-bottom: 0;}input[type=text],input[type=password],textarea,select {  display: inline-block;  width: 210px;  padding: 4px;  font-size: 13px;  font-weight: normal;  line-height: 18px;  height: 18px;  color: #808080;  border: 1px solid #ccc;  -webkit-border-radius: 3px;  -moz-border-radius: 3px;  border-radius: 3px;}select, input[type=file] {  height: 27px;  line-height: 27px;}textarea {  height: auto;}/* grey out placeholders */:-moz-placeholder {  color: #bfbfbf;}::-webkit-input-placeholder {  color: #bfbfbf;}input[type=text],input[type=password],select,textarea {  -webkit-transition: border linear 0.2s, box-shadow linear 0.2s;  -moz-transition: border linear 0.2s, box-shadow linear 0.2s;  transition: border linear 0.2s, box-shadow linear 0.2s;  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);}input[type=text]:focus, input[type=password]:focus, textarea:focus {  outline: none;  border-color: rgba(82, 168, 236, 0.8);  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);}/* buttons */button {  display: inline-block;  padding: 4px 14px;  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;  font-size: 13px;  line-height: 18px;  -webkit-border-radius: 4px;  -moz-border-radius: 4px;  border-radius: 4px;  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);  -moz-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);  background-color: #0064cd;  background-repeat: repeat-x;  background-image: -khtml-gradient(linear, left top, left bottom, from(#049cdb), to(#0064cd));  background-image: -moz-linear-gradient(top, #049cdb, #0064cd);  background-image: -ms-linear-gradient(top, #049cdb, #0064cd);  background-image: -webkit-gradient(linear, left top, left bottom, color-stop(0%, #049cdb), color-stop(100%, #0064cd));  background-image: -webkit-linear-gradient(top, #049cdb, #0064cd);  background-image: -o-linear-gradient(top, #049cdb, #0064cd);  background-image: linear-gradient(top, #049cdb, #0064cd);  color: #fff;  text-shadow: 0 -1px 0 rgba(0, 0, 0, 0.25);  border: 1px solid #004b9a;  border-bottom-color: #003f81;  -webkit-transition: 0.1s linear all;  -moz-transition: 0.1s linear all;  transition: 0.1s linear all;  border-color: #0064cd #0064cd #003f81;  border-color: rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.25);}button:hover {  color: #fff;  background-position: 0 -15px;  text-decoration: none;}button:active {  -webkit-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);  -moz-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);  box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);}button::-moz-focus-inner {  padding: 0;  border: 0;}/* table  */table {    border-spacing: 0;    border: 1px solid #ccc;}td, th{    border: 1px solid #ccc;    padding: 5px;}/* code syntax highlight.Documentation: http://www.mdcharm.com/documentation/code_syntax_highlighting.html#custom_your_own */pre .literal,pre .comment,pre .template_comment,pre .diff .header,pre .javadoc {    color: #008000;}pre .keyword,pre .css .rule .keyword,pre .winutils,pre .javascript .title,pre .nginx .title,pre .subst,pre .request,pre .status {    color: #0000FF;    font-weight: bold}pre .number,pre .hexcolor,pre .python .decorator,pre .ruby .constant {    color: #0000FF;}pre .string,pre .tag .value,pre .phpdoc,pre .tex .formula {    color: #D14}pre .title,pre .id {    color: #900;    font-weight: bold}pre .javascript .title,pre .lisp .title,pre .clojure .title,pre .subst {    font-weight: normal}pre .class .title,pre .haskell .type,pre .vhdl .literal,pre .tex .command {    color: #458;    font-weight: bold}pre .tag,pre .tag .title,pre .rules .property,pre .django .tag .keyword {    color: #000080;    font-weight: normal}pre .attribute,pre .variable,pre .lisp .body {    color: #008080}pre .regexp {    color: #009926}pre .class {    color: #458;    font-weight: bold}pre .symbol,pre .ruby .symbol .string,pre .lisp .keyword,pre .tex .special,pre .prompt {    color: #990073}pre .built_in,pre .lisp .title,pre .clojure .built_in {    color: #0086b3}pre .preprocessor,pre .pi,pre .doctype,pre .shebang,pre .cdata {    color: #999;    font-weight: bold}pre .deletion {    background: #fdd}pre .addition {    background: #dfd}pre .diff .change {    background: #0086b3}pre .chunk {    color: #aaa}pre .markdown .header {    color: #800;    font-weight: bold;}pre .markdown .blockquote {    color: #888;}pre .markdown .link_label {    color: #88F;}pre .markdown .strong {    font-weight: bold;}pre .markdown .emphasis {    font-style: italic;}
   </style>
   
   
</head>
<body>
    <h1><strong>Jifeng Dai</strong></h1>

<h1></h1>

<p>(代季峰)<br>
Executive Research Director, SenseTime Group Ltd</p>

<p><del>Principle Research Manager</del><br>
<del>Visual Computing Group, Microsoft Research Asia</del><br>
Email: daijifeng -at- sensetime.com<br>
Github: <a href="https://github.com/msracver">https://github.com/msracver</a> <a href="https://github.com/daijifeng001">https://github.com/daijifeng001</a></p>

<p><img src='./images/me.jpg', width='300'></p>

<h2>News</h2>

<p>I am founding a foundamental research team, and an applied research team at SenseTime Research. <strong>If you are interested in internship, Ph.D. program, foundamental research position, or applied researcher / developer positions related to computer vision or deep learning, please send me an <a href="mailto:daijifeng@sensetime.com">email</a>.</strong></p>

<h2>Short Bio</h2>

<p>Currently, I am an Executive Research Director at SenseTime Research. I am founding a foundamental research team, and an applied research team. My current research focus is on deep learning for high-level vision, especially for semantic segmentation and object detection.</p>

<p>Prior to that, I was a Principle Research Manager in Visual Computing Group at Microsoft Research Asia (MSRA), where I spent 5 wonderful years between 2014 and 2019. I got my Ph.D. degree from the Department of Automation, Tsinghua University in 2014, under the supervison of Professor <a href="https://www.tsinghua.edu.cn/publish/auen/1713/2011/20110506105532098625469/20110506105532098625469_.html">Jie Zhou</a>. During my Ph.D. study, I visited the VCLA lab of University of California, Los Angeles (UCLA) between 2012 and 2013, where I worked with Professor <a href="http://www.stat.ucla.edu/%7Esczhu/">Song-Chun Zhu</a> and Professor <a href="http://www.stat.ucla.edu/%7Eywu/">Ying-Nian Wu</a>. Before that, I got my B.S. degree from the Department of Automation, Tsinghua University in 2009.</p>

<h2>Academic Activity</h2>

<ul>
<li>Talk at <a href="https://www.objects365.org/workshop2019.html">Detection In the Wild Challenge Workshop 2019 at CVPR 2019</a> (<a href="https://www.objects365.org/slides/STN_DCN_short.pdf">slides</a>)</li>
<li>Talk at <a href="https://instancetutorial.github.io/">ICCV 2017 Tutorial on Instance-level Recognition</a> (<a href="http://www.jifengdai.org/slides/Jifeng_FlowBasedVideoRecognition.pdf">slides</a>)</li>
<li>Associate Editor of IJCV</li>
<li>Senior PC member: AAAI 2018</li>
<li>Oral &amp; spotlight session chair: CVPR 2016</li>
<li>Journal reviewer: TPAMI, IJCV, CVIU, TIP, TMM, etc</li>
<li>Conference reviewer: CVPR, ICCV, ECCV, ICLR, etc</li>
</ul>

<h2>Publication (<a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ&amp;hl=zh-CN">Google Scholar</a>)</h2>

<p>(* Interns, <sup>+</sup> Equal contribution)</p>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1910.02940">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a> <br />
Hang Gao*<sup>+</sup>, Xizhou Zhu*<sup>+</sup>, Steve Lin, and <strong>Jifeng Dai</strong>  <br />
Arxiv Tech Report, 2019. <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1908.08530">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a> <br />
Weijie Su*<sup>+</sup>, Xizhou Zhu*<sup>+</sup>,  Yue Cao, Bin Li, Lewei Lu, Furu Wei, and <strong>Jifeng Dai</strong>  <br />
Arxiv Tech Report, 2019. <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1904.05873">An Empirical Study of Spatial Attention Mechanisms in Deep Networks</a> <br />
Xizhou Zhu*<sup>+</sup>, Dazhi Cheng*<sup>+</sup>, Zheng Zhang<sup>+</sup>, Steve Lin, and <strong>Jifeng Dai</strong>  <br />
International Conference on Computer Vision (ICCV), 2019. <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1811.11168">Deformable ConvNets v2: More Deformable, Better Results</a> <br />
Xizhou Zhu*, Han Hu, Steve Lin, and <strong>Jifeng Dai</strong>  <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.<br />
<a href="https://github.com/msracver/Deformable-ConvNets">The updated operators utilized in Deformable ConvNets v2 are provided here!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1811.11167">Integrated Object Detection and Tracking with Tracklet-Conditioned Detection</a> <br />
Zheng Zhang<sup>+</sup>, Dazhi Cheng*<sup>+</sup>, Xizhou Zhu*<sup>+</sup>, Steve Lin, and <strong>Jifeng Dai</strong>  <br />
Arxiv Tech Report, 2018.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1804.05830">Towards High Performance Video Object Detection for Mobiles</a> <br />
Xizhou Zhu*, <strong>Jifeng Dai</strong>, Xingchi Zhu*, Yichen Wei, and Lu Yuan  <br />
Arxiv Tech Report, 2018.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1803.07066">Learning Region Features for Object Detection</a> <br />
Jiayuan Gu*, Han Hu, Liwei Wang, Yichen Wei, and <strong>Jifeng Dai</strong>  <br />
European Conference on Computer Vision (ECCV), 2018.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1711.11575">Relation Networks for Object Detection</a> <br />
Han Hu<sup>+</sup>, Jiayuan Gu*<sup>+</sup>, Zheng Zhang<sup>+</sup>, <strong>Jifeng Dai</strong>, and Yichen Wei <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. (<strong>Oral</strong>) <br />
<a href="https://github.com/msracver/Relation-Networks-for-Object-Detection">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1711.11577">Towards High Performance Video Object Detection</a> <br />
Xizhou Zhu*, <strong>Jifeng Dai</strong>, Lu Yuan, and Yichen Wei <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. (<strong>Spotlight</strong>) <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1703.06211">Deformable Convolutional Networks</a> <br />
<strong>Jifeng Dai</strong><sup>+</sup>, Haozhi Qi*<sup>+</sup>, Yuwen Xiong*<sup>+</sup>, Yi Li*<sup>+</sup>, Guodong Zhang*<sup>+</sup>, Han Hu, and Yichen Wei <br />
International Conference on Computer Vision (ICCV), 2017. (<strong>Oral</strong>) <br />
<a href="https://github.com/msracver/Deformable-ConvNets">Code is available!</a> <br />
Slides of <a href="http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf">ICCV Oral</a>, <a href="http://presentations.cocodataset.org/COCO17-Detect-MSRA.pdf">COCO2017 workshop</a><br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1703.10025">Flow-Guided Feature Aggregation for Video Object Detection</a> <br />
Xizhou Zhu*, Yujie Wang*, <strong>Jifeng Dai</strong>, Lu Yuan, and Yichen Wei <br />
International Conference on Computer Vision (ICCV), 2017. <br />
<a href="https://github.com/msracver/Flow-Guided-Feature-Aggregation">Code is available!</a> <br />
<a href="https://www.youtube.com/watch?v=R2h3DbTPvVg">Video</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1611.07709">Fully Convolutional Instance-aware Semantic Segmentation</a> <br />
Yi Li*<sup>+</sup>, Haozhi Qi*<sup>+</sup>, <strong>Jifeng Dai</strong>, Xiangyang Ji, and Yichen Wei <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. (<strong>Spotlight</strong>)<br />
1-st place winner of the COCO 2016 segmentation challenge  <br />
0.24 sec/image test speed (using ResNet-101 net) <br />
<a href="https://github.com/msracver/FCIS">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1611.07715">Deep Feature Flow for Video Recognition</a> <br />
Xizhou Zhu*, Yuwen Xiong*, <strong>Jifeng Dai</strong>, Lu Yuan, and Yichen Wei <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. <br />
<a href="https://github.com/msracver/Deep-Feature-Flow">Code is available!</a> <br />
<a href="https://www.youtube.com/watch?v=J0rMHE6ehGw">Video</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://arxiv.org/abs/1605.06409">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a> <br />
<strong>Jifeng Dai</strong>, Yi Li, Kaiming He, and Jian Sun <br />
Neural Information Processing Systems (NIPS), 2016. <br />
<a href="https://github.com/daijifeng001/R-FCN">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://arxiv.org/abs/1603.08678">Instance-sensitive Fully Convolutional Networks</a> <br />
<strong>Jifeng Dai</strong>, Kaiming He, Yi Li, Shaoqing Ren, and Jian Sun <br />
European Conference on Computer Vision (ECCV), 2016.</p>
</blockquote>

<!-- -->

<blockquote>
<p><a href="http://arxiv.org/abs/1604.05144">ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation</a> <br />
Di Lin*, <strong>Jifeng Dai</strong>, Jiaya Jia, Kaiming He, and Jian Sun <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. (<strong>Oral</strong>) <br />
<a href="http://www.jifengdai.org/downloads/scribble_sup/">PASCAL-scribble dataset is available here</a>!</p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://arxiv.org/abs/1512.04412">Instance-aware Semantic Segmentation via Multi-task Network Cascades</a> <br />
<strong>Jifeng Dai</strong>, Kaiming He, and Jian Sun <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. (<strong>Oral</strong>) <br />
1-st place winner of the COCO 2015 segmentation challenge  <br />
0.36 sec/image test speed (using VGG-16 net) <br />
<a href="https://github.com/daijifeng001/MNC">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://arxiv.org/abs/1503.01640">BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</a> <br />
<strong>Jifeng Dai</strong>, Kaiming He, and Jian Sun <br />
International Conference on Computer Vision (ICCV), 2015.</p>
</blockquote>

<!-- -->

<blockquote>
<p><a href="http://arxiv.org/abs/1412.1283">Convolutional Feature Masking for Joint Object and Stuff Segmentation</a> <br />
<strong>Jifeng Dai</strong>, Kaiming He, and Jian Sun <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.</p>
</blockquote>

<!-- -->  

<blockquote>
<p><a href="http://arxiv.org/abs/1412.6296">Generative Modeling of Convolutional Neural Networks</a> <br />
<strong>Jifeng Dai</strong>, Yang Lu, and Ying-Nian Wu <br />
International Conference on Learning Representations (ICLR), 2015. <br />
<a href="http://www.stat.ucla.edu/%7Eyang.lu/Project/generativeCNN/main.html">project page</a></p>
</blockquote>

<!-- -->   

<blockquote>
<p><a href="http://www.jifengdai.org/publications/CVPR_2014.pdf">Unsupervised Learning of Dictionaries of Hierarchical Compositional Models</a> <br />
<strong>Jifeng Dai</strong>, Yi Hong, Wenze Hu, Song-Chun Zhu, and Ying-Nian Wu <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014. <br />
<a href="http://www.stat.ucla.edu/%7Ejifeng.dai/research/HCM.html">project page</a>  </p>
</blockquote>

<!-- -->  

<blockquote>
<p><a href="http://www.jifengdai.org/publications/ICCV_Cosegmentation_2013.pdf">Cosegmentation and Cosketch by Unsupervised Learning</a> <br />
<strong>Jifeng Dai</strong>, Ying-Nian Wu, Jie Zhou, and Song-Chun Zhu <br />
International Conference on Computer Vision (ICCV), 2013. <br />
<a href="http://www.stat.ucla.edu/%7Ejifeng.dai/research/CosegmentationCosketch.html">project page</a> </p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://www.jifengdai.org/publications/ICPR_Mining_2012.pdf">Mining Sub-categories For Object Detection</a> <br />
<strong>Jifeng Dai</strong>, Jianjiang Feng, and Jie Zhou <br />
International Conference on Pattern Recognition (ICPR), 2012. (<strong>Oral</strong>)</p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://www.jifengdai.org/publications/Newsletter_Ridge_2013.pdf">Ridge Based Palmprint Matching</a> <br />
<strong>Jifeng Dai</strong>, Jianjiang Feng, and Jie Zhou <br />
IEEE Biometrics Council Newsletter, 7:4-5, 2013. (Invited paper)  </p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://www.jifengdai.org/publications/PAMI_Robust_2012.pdf">Robust and Efficient Ridge-Based Palmprint Matching</a> <br />
<strong>Jifeng Dai</strong>, Jianjiang Feng, and Jie Zhou  <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 34(8):1618–1632, 2012.</p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://www.jifengdai.org/publications/PAMI_MultiFeature_2011.pdf">Multi-feature Based High-resolution Palmprint Recognition</a> <br />
<strong>Jifeng Dai</strong>, and Jie Zhou <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 33(5):945–957, 2011.</p>
</blockquote>

<h2>Competitions</h2>

<ul>
<li>3rd place in COCO object detection challenge 2017</li>
<li>1st place in COCO instance segmentation challenge 2016</li>
<li>1st place in COCO object detection &amp; instance segmentation challenge 2015</li>
</ul>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82196129-1', 'auto');
  ga('send', 'pageview');

</script>

</body>
</html>
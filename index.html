<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
   <meta http-equiv="content-type" content="text/html;charset=UTF-8">
   <style type="text/css">
   /*CSS stylesheet is based on killwing's flavored markdown style:https://gist.github.com/2937864*/body{    margin: 0 auto;    font: 13px/1.231 Helvetica, Arial, sans-serif;    color: #444444;    line-height: 1;    max-width: 960px;    padding: 5px;}h1, h2, h3, h4 {    color: #111111;    font-weight: 400;}h1, h2, h3, h4, h5, p {    margin-bottom: 16px;    padding: 0;}h1 {    font-size: 28px;}h2 {    font-size: 22px;    margin: 20px 0 6px;}h3 {    font-size: 21px;}h4 {    font-size: 18px;}h5 {    font-size: 16px;}a {    color: #0099ff;    margin: 0;    padding: 0;    vertical-align: baseline;}a:link,a:visited{ text-decoration:none;}a:hover{ text-decoration:underline;}ul, ol {    padding: 0;    margin: 0;}li {    line-height: 24px;    margin-left: 44px;}li ul, li ul {    margin-left: 24px;}ul, ol {    font-size: 14px;    line-height: 20px;    max-width: 540px;}p {    font-size: 14px;    line-height: 20px;    max-width: 540px;    margin-top: 3px;}pre {    padding: 0px 4px;    max-width: 800px;    white-space: pre-wrap;    font-family: Consolas, Monaco, Andale Mono, monospace;    line-height: 1.5;    font-size: 13px;    border: 1px solid #ddd;    background-color: #f7f7f7;    border-radius: 3px;}code {    font-family: Consolas, Monaco, Andale Mono, monospace;    line-height: 1.5;    font-size: 13px;    border: 1px solid #ddd;    background-color: #f7f7f7;    border-radius: 3px;}pre code {    border: 0px;}aside {    display: block;    float: right;    width: 390px;}blockquote {    border-left:.5em solid #40AA53;    padding: 0 2em;    margin-left:0;    max-width: 476px;}blockquote  cite {    font-size:14px;    line-height:20px;    color:#bfbfbf;}blockquote cite:before {    content: '\2014 \00A0';}blockquote p {      color: #666;    max-width: 460px;}hr {    height: 1px;    border: none;    border-top: 1px dashed #0066CC}button,input,select,textarea {  font-size: 100%;  margin: 0;  vertical-align: baseline;  *vertical-align: middle;}button, input {  line-height: normal;  *overflow: visible;}button::-moz-focus-inner, input::-moz-focus-inner {  border: 0;  padding: 0;}button,input[type="button"],input[type="reset"],input[type="submit"] {  cursor: pointer;  -webkit-appearance: button;}input[type=checkbox], input[type=radio] {  cursor: pointer;}/* override default chrome & firefox settings */input:not([type="image"]), textarea {  -webkit-box-sizing: content-box;  -moz-box-sizing: content-box;  box-sizing: content-box;}input[type="search"] {  -webkit-appearance: textfield;  -webkit-box-sizing: content-box;  -moz-box-sizing: content-box;  box-sizing: content-box;}input[type="search"]::-webkit-search-decoration {  -webkit-appearance: none;}label,input,select,textarea {  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;  font-size: 13px;  font-weight: normal;  line-height: normal;  margin-bottom: 18px;}input[type=checkbox], input[type=radio] {  cursor: pointer;  margin-bottom: 0;}input[type=text],input[type=password],textarea,select {  display: inline-block;  width: 210px;  padding: 4px;  font-size: 13px;  font-weight: normal;  line-height: 18px;  height: 18px;  color: #808080;  border: 1px solid #ccc;  -webkit-border-radius: 3px;  -moz-border-radius: 3px;  border-radius: 3px;}select, input[type=file] {  height: 27px;  line-height: 27px;}textarea {  height: auto;}/* grey out placeholders */:-moz-placeholder {  color: #bfbfbf;}::-webkit-input-placeholder {  color: #bfbfbf;}input[type=text],input[type=password],select,textarea {  -webkit-transition: border linear 0.2s, box-shadow linear 0.2s;  -moz-transition: border linear 0.2s, box-shadow linear 0.2s;  transition: border linear 0.2s, box-shadow linear 0.2s;  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);}input[type=text]:focus, input[type=password]:focus, textarea:focus {  outline: none;  border-color: rgba(82, 168, 236, 0.8);  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);}/* buttons */button {  display: inline-block;  padding: 4px 14px;  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;  font-size: 13px;  line-height: 18px;  -webkit-border-radius: 4px;  -moz-border-radius: 4px;  border-radius: 4px;  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);  -moz-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);  background-color: #0064cd;  background-repeat: repeat-x;  background-image: -khtml-gradient(linear, left top, left bottom, from(#049cdb), to(#0064cd));  background-image: -moz-linear-gradient(top, #049cdb, #0064cd);  background-image: -ms-linear-gradient(top, #049cdb, #0064cd);  background-image: -webkit-gradient(linear, left top, left bottom, color-stop(0%, #049cdb), color-stop(100%, #0064cd));  background-image: -webkit-linear-gradient(top, #049cdb, #0064cd);  background-image: -o-linear-gradient(top, #049cdb, #0064cd);  background-image: linear-gradient(top, #049cdb, #0064cd);  color: #fff;  text-shadow: 0 -1px 0 rgba(0, 0, 0, 0.25);  border: 1px solid #004b9a;  border-bottom-color: #003f81;  -webkit-transition: 0.1s linear all;  -moz-transition: 0.1s linear all;  transition: 0.1s linear all;  border-color: #0064cd #0064cd #003f81;  border-color: rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.25);}button:hover {  color: #fff;  background-position: 0 -15px;  text-decoration: none;}button:active {  -webkit-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);  -moz-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);  box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);}button::-moz-focus-inner {  padding: 0;  border: 0;}/* table  */table {    border-spacing: 0;    border: 1px solid #ccc;}td, th{    border: 1px solid #ccc;    padding: 5px;}/* code syntax highlight.Documentation: http://www.mdcharm.com/documentation/code_syntax_highlighting.html#custom_your_own */pre .literal,pre .comment,pre .template_comment,pre .diff .header,pre .javadoc {    color: #008000;}pre .keyword,pre .css .rule .keyword,pre .winutils,pre .javascript .title,pre .nginx .title,pre .subst,pre .request,pre .status {    color: #0000FF;    font-weight: bold}pre .number,pre .hexcolor,pre .python .decorator,pre .ruby .constant {    color: #0000FF;}pre .string,pre .tag .value,pre .phpdoc,pre .tex .formula {    color: #D14}pre .title,pre .id {    color: #900;    font-weight: bold}pre .javascript .title,pre .lisp .title,pre .clojure .title,pre .subst {    font-weight: normal}pre .class .title,pre .haskell .type,pre .vhdl .literal,pre .tex .command {    color: #458;    font-weight: bold}pre .tag,pre .tag .title,pre .rules .property,pre .django .tag .keyword {    color: #000080;    font-weight: normal}pre .attribute,pre .variable,pre .lisp .body {    color: #008080}pre .regexp {    color: #009926}pre .class {    color: #458;    font-weight: bold}pre .symbol,pre .ruby .symbol .string,pre .lisp .keyword,pre .tex .special,pre .prompt {    color: #990073}pre .built_in,pre .lisp .title,pre .clojure .built_in {    color: #0086b3}pre .preprocessor,pre .pi,pre .doctype,pre .shebang,pre .cdata {    color: #999;    font-weight: bold}pre .deletion {    background: #fdd}pre .addition {    background: #dfd}pre .diff .change {    background: #0086b3}pre .chunk {    color: #aaa}pre .markdown .header {    color: #800;    font-weight: bold;}pre .markdown .blockquote {    color: #888;}pre .markdown .link_label {    color: #88F;}pre .markdown .strong {    font-weight: bold;}pre .markdown .emphasis {    font-style: italic;}
   </style>
   
   
</head>
<body>
    <h1><strong>Jifeng Dai</strong></h1>

<h1></h1>

<p>(代季峰)<br>
Associate Professor, Department of Electronic Engineering, Tsinghua University
Adjunct Leading Scientist, Shanghai AI Laboratory</p>

<p>Email: daijifeng -at- tsinghua.edu.cn</p>

<p>Group Webpage: <a href="https://fundamentalvision.github.io/">https://fundamentalvision.github.io/</a></p>

<p>Github: <a href="https://github.com/fundamentalvision">https://github.com/fundamentalvision</a> <a href="https://github.com/OpenGVLab">https://github.com/OpenGVLab</a> <a href="https://github.com/msracver">https://github.com/msracver</a> <a href="https://github.com/daijifeng001">https://github.com/daijifeng001</a> </p>

<p><img src='./images/me.jpg', width='500'></p>

<h2>News</h2>

<p>My lab at Tsinghua University is now hiring. <strong>If you are interested in internship, Ph.D. program, postdoctoral positions related to computer vision or deep learning, please send me an <a href="mailto:daijifeng@tsinghua.edu.cn">email</a>.</strong></p>

<p><strong>If you are intested in internship or job position at Shanghai AI Laboratory related to my research field, please send me an <a href="mailto:daijifeng@tsinghua.edu.cn">email</a> as well.</strong></p>

<h2>Research Highlights</h2>

<p>Please try <a href="https://internvl.opengvlab.com/">InternVL</a>, a leading open-source multi-modal foundation model</p>

<p><a href="https://arxiv.org/abs/2211.05778">InternImage</a> ranks <a href="https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/">10th of the most influential papers in CVPR 2023</a></p>

<p><a href="https://arxiv.org/abs/2212.10156">UniAD</a> won the <a href="https://cvpr2023.thecvf.com/Conferences/2023/Awards">Best Paper Award of CVPR 2023</a></p>

<p><a href="https://arxiv.org/abs/2203.17270">BEVFormer</a> ranks <a href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/">6th of the most influential papers in ECCV 2022</a></p>

<p><a href="https://arxiv.org/abs/2010.04159">Deformable DETR</a> ranks <a href="https://www.paperdigest.org/2022/02/most-influential-iclr-papers-2022-02/">2nd of the most influential papers in ICLR 2021</a></p>

<p><a href="https://arxiv.org/abs/1908.08530">VL-BERT</a> ranks <a href="https://www.paperdigest.org/2022/02/most-influential-iclr-papers-2022-02/">7th of the most influential papers in ICLR 2020</a></p>

<p><a href="https://arxiv.org/abs/1703.06211">Deformable ConvNets</a> ranks <a href="https://www.paperdigest.org/2022/05/most-influential-iccv-papers-2022-05/">6th of the most influential papers in ICCV 2017</a></p>

<p><a href="https://arxiv.org/abs/1605.06409">R-FCN</a> ranks <a href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/">3rd of the most influential papers in NIPS 2016</a></p>

<h2>Short Bio</h2>

<p>Currently, I am an Associate Professor at Department of Electronic Engineering of Tsinghua University. My current research focus is on learning intelligent models from visual data for understanding the complex world.</p>

<p>Prior to that, I was an Executive Research Director at SenseTime Research, headed by Professor <a href="http://www.ee.cuhk.edu.hk/%7Exgwang/">Xiaogang Wang</a>, between 2019 and 2022. I was a Principal Research Manager in Visual Computing Group at Microsoft Research Asia (MSRA) between 2014 and 2019, headed by Dr. <a href="http://www.jiansun.org/">Jian Sun</a>. </p>

<p>I got my Ph.D. degree from the Department of Automation, Tsinghua University in 2014, under the supervison of Professor <a href="https://www.tsinghua.edu.cn/publish/auen/1713/2011/20110506105532098625469/20110506105532098625469_.html">Jie Zhou</a>. During my Ph.D. study, I visited the VCLA lab of University of California, Los Angeles (UCLA) between 2012 and 2013, where I worked with Professor <a href="http://www.stat.ucla.edu/%7Esczhu/">Song-Chun Zhu</a> and Professor <a href="http://www.stat.ucla.edu/%7Eywu/">Ying-Nian Wu</a>. Before that, I got my B.S. degree from the Department of Automation, Tsinghua University in 2009, GPA ranking 2/160+.</p>

<h2>Academic Activity</h2>

<ul>
<li>Associate Editor of TPAMI</li>
<li>Editorial Board Member of IJCV</li>
<li>Area Chair of ECCV 2020, CVPR 2021, WACV2021, CVPR 2023, ICCV 2023, NeurIPS 2023, ICLR 2024, ECCV 2024, NeurIPS 2024</li>
<li>Senior PC member: AAAI 2018, 2023</li>
<li>Publicity chair: ICCV 2019</li>
<li>Oral &amp; spotlight session chair: CVPR 2016</li>
<li>Journal reviewer: TPAMI, IJCV, CVIU, TIP, TMM, etc</li>
<li>Conference reviewer: CVPR, ICCV, ECCV, ICLR, etc</li>
<li>Talk at <a href="https://openmmlab.com/cvpr2021-tutorial/">Tutorial on OpenMMLab at CVPR 2021</a></li>
<li>Talk at CAAI on VL-BERT (<a href="http://www.jifengdai.org/slides/VL-BERT.pdf">slides</a>)</li>
<li>Talk at <a href="http://www.ee.oulu.fi/%7Elili/CEFRLatICCV2019.html">4th International Workshop on
Compact and Efficient Feature Representation and Learning in Computer Vision at ICCV 2019</a> (<a href="https://1drv.ms/b/s!Am-5JzdW2XHzhrx7JgHjLvKepvh32A?e=ye2hse">slides</a>)</li>
<li>Talk at <a href="https://www.objects365.org/workshop2019.html">Detection In the Wild Challenge Workshop 2019 at CVPR 2019</a> (<a href="https://www.objects365.org/slides/STN_DCN_short.pdf">slides</a>)</li>
<li>Talk at <a href="https://instancetutorial.github.io/">ICCV 2017 Tutorial on Instance-level Recognition</a> (<a href="http://www.jifengdai.org/slides/Jifeng_FlowBasedVideoRecognition.pdf">slides</a>)</li>
</ul>

<h2>Publication (<a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ&amp;hl=zh-CN">Google Scholar</a>)</h2>

<p>(* Interns &amp; Students, <sup>+</sup> Equal contribution)</p>

<!-- --> 

<blockquote>
<p><a href="https://www.computer.org/csdl/journal/tp/5555/01/10791908/22ABgP6PlUQ">BEVFormer: Learning Bird&#39;s-Eye-View Representation From LiDAR-Camera Via Spatiotemporal Transformers</a> <br />
Zhiqi Li*<sup>+</sup>, Wenhai Wang<sup>+</sup>, Hongyang Li<sup>+</sup>, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, <strong>Jifeng Dai</strong> <br />
TPAMI 2025.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2211.05781">Demystify Transformers &amp; Convolutions in Modern Image Deep Networks</a> <br />
Xiaowei Hu<sup>+</sup>, Min Shi*<sup>+</sup>, Weiyun Wang*<sup>+</sup>, Sitong Wu*<sup>+</sup>, Linjie Xing, Wenhai Wang, Xizhou Zhu, Lewei Lu, Jie Zhou, Xiaogang Wang, Yu Qiao, <strong>Jifeng Dai</strong> <br />
TPAMI 2025.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2412.09604">SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding</a> <br />
Hao Li*<sup>+</sup>, Changyao Tian*<sup>+</sup>, Jie Shao*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, <strong>Jifeng Dai</strong> <br />
CVPR 2025.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2412.09613">PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models</a> <br />
Chenyu Yang*<sup>+</sup>, Xuan Dong*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Weijie Su<sup>+</sup>, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, <strong>Jifeng Dai</strong> <br />
CVPR 2025.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2412.16158">HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding</a> <br />
Chenxin Tao*<sup>+</sup>, Shiqian Su*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Chenyu Zhang, Zhe Chen, Jiawen Liu, Wenhai Wang, Lewei Lu, Gao Huang, Yu Qiao, <strong>Jifeng Dai</strong> <br />
CVPR 2025.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2406.08394">VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks</a> <br />
Jiannan Wu*<sup>+</sup>, Muyan Zhong*<sup>+</sup>, Sen Xing*<sup>+</sup>, Zeqiang Lai*<sup>+</sup>, Zhaoyang Liu*<sup>+</sup>, Wenhai Wang<sup>+</sup>, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Ping Luo, Yu Qiao, <strong>Jifeng Dai</strong> <br />
NeurIPS 2024.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2406.08418">OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text</a> <br />
Qingyun Li*<sup>+</sup>, Zhe Chen*<sup>+</sup>, Weiyun Wang*<sup>+</sup>, Wenhai Wang<sup>+</sup>, Shenglong Ye*<sup>+</sup>, Zhenjiang Jin*<sup>+</sup>, Guanzhou Chen*<sup>+</sup>, Yinan He<sup>+</sup>, Zhangwei Gao*<sup>+</sup>, Erfei Cui*<sup>+</sup>, Jiashuo Yu*<sup>+</sup>, Hao Tian<sup>+</sup>, Jiasheng Zhou<sup>+</sup>, Chao Xu<sup>+</sup>, Bin Wang<sup>+</sup>, Xingjian Wei<sup>+</sup>, Wei Li<sup>+</sup>, Wenjian Zhang<sup>+</sup>, Bo Zhang<sup>+</sup>, Pinlong Cai<sup>+</sup>, Licheng Wen<sup>+</sup>, Xiangchao Yan<sup>+</sup>, Pei Chu<sup>+</sup>, Yi Wang<sup>+</sup>, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, <strong>Jifeng Dai</strong> <br />
ICLR 2025.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2406.07543">Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning</a> <br />
Chenyu Yang*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Jinguo Zhu*<sup>+</sup>, Weijie Su, Junjie Wang, Xuan Dong, Wenhai Wang, Lewei Lu, Bin Li, Jie Zhou, Yu Qiao, <strong>Jifeng Dai</strong> <br />
NeurIPS 2024.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2406.04342">Learning 1D Causal Visual Representation with De-focus Attention Networks</a> <br />
Chenxin Tao*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Shiqian Su*<sup>+</sup>, Lewei Lu, Changyao Tian, Xuan Luo, Gao Huang, Hongsheng Li, Yu Qiao, Jie Zhou,  <strong>Jifeng Dai</strong> <br />
NeurIPS 2024.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2406.04330">Parameter-Inverted Image Pyramid Networks</a> <br />
Xizhou Zhu<sup>+</sup>, Xue Yang<sup>+</sup>, Zhaokai Wang*<sup>+</sup>, Hao Li, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, <strong>Jifeng Dai</strong> <br />
NeurIPS 2024. (Spotlight)<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2402.19474">The All-Seeing Project V2: Towards General Relation Comprehension of the Open World</a> <br />
Weiyun Wang*, Yiming Ren*, Haowen Luo*, Tiantong Li*, Chenxiang Yan*, Zhe Chen*, Wenhai Wang, Qingyun Li*, Lewei Lu, Xizhou Zhu, Yu Qiao, <strong>Jifeng Dai</strong> <br />
ECCV 2024.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2310.17796">ControlLLM: Augment Language Models with Tools by Searching on Graphs</a> <br />
Zhaoyang Liu*<sup>+</sup>, Zeqiang Lai*<sup>+</sup>, Zhangwei Gao, Erfei Cui, Ziheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, <strong>Jifeng Dai</strong>, Wenhai Wang<br />
ECCV 2024.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2401.06197">Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications</a> <br />
Yuwen Xiong*<sup>+</sup>, Zhiqi Li*<sup>+</sup>, Yuntao Chen<sup>+</sup>, Feng Wang*<sup>+</sup>, Xizhou Zhu, Jiapeng Luo, Wenhai Wang, Tong Lu, Hongsheng Li, Yu Qiao, Lewei Lu, Jie Zhou, <strong>Jifeng Dai</strong> <br />
CVPR 2024. (Highlight) <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2401.10208">MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer</a> <br />
Changyao Tian*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Yuwen Xiong*<sup>+</sup>, Weiyun Wang*, Zhe Chen*, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, <strong>Jifeng Dai</strong> <br />
Arxiv Tech Report, 2024.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2312.14238">InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</a> <br />
Zhe Chen*, Jiannan Wu*, Wenhai Wang, Weijie Su*,  Guo Chen, Sen Xing*, Zhong Muyan*, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, <strong>Jifeng Dai</strong> <br />
CVPR 2024. (Oral) <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2312.09245">DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving</a> <br />
Wenhai Wang<sup>+</sup>, Jiangwei Xie<sup>+</sup>, ChuanYang Hu<sup>+</sup>, Haoming Zou<sup>+</sup>, Jianan Fan<sup>+</sup>, Wenwen Tong<sup>+</sup>, Yang Wen<sup>+</sup>, Silei Wu<sup>+</sup>, Hanming Deng<sup>+</sup>, Zhiqi Li<sup>+</sup>, Hao Tian, Lewei Lu, Xizhou Zhu, Xiaogang Wang, Yu Qiao, <strong>Jifeng Dai</strong> <br />
Arxiv Tech Report, 2023.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2312.09238">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a> <br />
Hao Li<sup>+</sup>, Xue Yang<sup>+</sup>, Zhaokai Wang<sup>+</sup>, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, <strong>Jifeng Dai</strong><br />
CVPR 2024.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2308.01907">The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World</a> <br />
Weiyun Wang*<sup>+</sup>, Min Shi*<sup>+</sup>, Qingyun Li*<sup>+</sup>, Wenhai Wang<sup>+</sup>, Zhenhang Huang<sup>+</sup>, Linjie Xing<sup>+</sup>, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, Yushi Chen, Tong Lu, <strong>Jifeng Dai</strong>, Yu Qiao <br />
ICLR 2024.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2305.17144">Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory</a> <br />
Xizhou Zhu<sup>+</sup>, Yuntao Chen<sup>+</sup>, Hao Tian<sup>+</sup>, Chenxin Tao*<sup>+</sup>, Weijie Su*<sup>+</sup>, Chenyu Yang*<sup>+</sup>, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, <strong>Jifeng Dai</strong> <br />
Arxiv Tech Report, 2023.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2305.11175">VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks</a> <br />
Wenhai Wang<sup>+</sup>, Zhe Chen*<sup>+</sup>, Xiaokang Chen*<sup>+</sup>, Jiannan Wu*<sup>+</sup>, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, <strong>Jifeng Dai</strong> <br />
NeurIPS 2023.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2212.10156">Planning-oriented Autonomous Driving</a> <br />
Yihan Hu<sup>+</sup>, Jiazhi Yang<sup>+</sup>, Li Chen<sup>+</sup>, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, <strong>Jifeng Dai</strong>, Yu Qiao, Hongyang Li <br />
CVPR 2023. (CVPR Best Paper Award)<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2211.10439">BEVFormer v2: Adapting Modern Image Backbones to Bird&#39;s-Eye-View Recognition via Perspective Supervision</a> <br />
Chenyu Yang*<sup>+</sup>, Yuntao Chen<sup>+</sup>, Hao Tian<sup>+</sup>, Chenxin Tao*, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, Jie Zhou, <strong>Jifeng Dai</strong> <br />
CVPR 2023. (Highlight)<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2211.09807">Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information</a> <br />
Weijie Su*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Chenxin Tao*<sup>+</sup>, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, <strong>Jifeng Dai</strong> <br />
CVPR 2023. (Highlight)<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2211.09808">Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks</a> <br />
Hao Li*<sup>+</sup>, Jingguo Zhu*<sup>+</sup>, Xiaohu Jiang*<sup>+</sup>, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, <strong>Jifeng Dai</strong> <br />
CVPR 2023. (Highlight)<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2211.05778">InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</a> <br />
Wenhai Wang<sup>+</sup>, <strong>Jifeng Dai</strong><sup>+</sup>, Zhe Chen*<sup>+</sup>, Zhenhang Huang<sup>+</sup>, Zhiqi Li*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao <br />
CVPR 2023. (Highlight)<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2206.01204">Siamese Image Modeling for Self-Supervised Vision Representation Learning</a> <br />
Chenxin Tao*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Gao Huang, Yu Qiao, Xiaogang Wang, <strong>Jifeng Dai</strong> <br />
CVPR 2023. (Highlight)<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2206.04674">Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs</a> <br />
Jingguo Zhu*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, <strong>Jifeng Dai</strong> <br />
NeurIPS, 2022.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2203.17270">BEVFormer: Learning Bird&#39;s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</a> <br />
Zhiqi Li*<sup>+</sup>, Wenhai Wang<sup>+</sup>, Hongyang Li<sup>+</sup>, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, <strong>Jifeng Dai</strong> <br />
ECCV 2022.<br />
<a href="https://github.com/zhiqi-li/BEVFormer">Code is available!</a><br />
<a href="https://waymo.com/open/challenges/2022/3d-camera-only-detection/">BEVFormer won the 1-st place of  Waymo 2022 3D Camera-Only Detection Task</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2111.13579">VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition</a> <br />
Changyao Tian*<sup>+</sup>, Wenhai Wang<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Xiaogang Wang, <strong>Jifeng Dai</strong>, Yu Qiao <br />
ECCV 2022.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2112.05141">Exploring the Equivalence of Siamese Self-Supervised Learning via A Unified Gradient Framework</a> <br />
Chenxin Tao*<sup>+</sup>, Honghui Wang*<sup>+</sup>, Xizhou Zhu<sup>+</sup>,  Jiahua Dong, Shiji Song, Gao Huang, <strong>Jifeng Dai</strong> <br />
CVPR 2022.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2112.01522">Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks</a> <br />
Xizhou Zhu<sup>+</sup>, Jinguo Zhu*<sup>+</sup>, Hao Li*<sup>+</sup>, Xiaoshi Wu*<sup>+</sup>, Xiaogang Wang, Hongsheng Li, Xiaohua Wang, <strong>Jifeng Dai</strong> <br />
CVPR 2022.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2103.14026">AutoLoss-Zero: Searching Loss Functions from Scratch for Generic Tasks</a> <br />
Hao Li*<sup>+</sup>, Tianwen Fu*<sup>+</sup>, <strong>Jifeng Dai</strong> ,Hongsheng Li, Gao Huang, and Xizhou Zhu <br />
CVPR 2022.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://openreview.net/forum?id=hLTZCN7f3M-">Searching Parameterized AP Loss for Object Detection</a> <br />
Chenxin Tao*<sup>+</sup>, Zizhang Li*<sup>+</sup>, Xizhou Zhu<sup>+</sup>, Gao Huang, Yong Liu, <strong>Jifeng Dai</strong> <br />
NeurIPS 2021.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2011.12953">Unsupervised Object Detection with LiDAR Clues</a> <br />
Hao Tian*<sup>+</sup>, Yuntao Chen*<sup>+</sup>, <strong>Jifeng Dai</strong> , Zhaoxiang Zhang, and Xizhou Zhu <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2010.07930">Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation</a> <br />
Hao Li*<sup>+</sup>, Chenxin Tao*<sup>+</sup>, Xizhou Zhu, Xiaogang Wang, Gao Huang, and <strong>Jifeng Dai</strong>  <br />
International Conference on Learning Representations (ICLR), 2021. <br />
<a href="https://github.com/fundamentalvision/Auto-Seg-Loss">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/2010.04159">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a> <br />
Xizhou Zhu<sup>+</sup>, Weijie Su*<sup>+</sup>, Lewei Lu, Bin Li, Xiaogang Wang, and <strong>Jifeng Dai</strong>  <br />
International Conference on Learning Representations (ICLR), 2021.(<strong>Oral</strong>) <br />
<a href="https://github.com/fundamentalvision/Deformable-DETR">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1910.02940">Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</a> <br />
Hang Gao*<sup>+</sup>, Xizhou Zhu*<sup>+</sup>, Steve Lin, and <strong>Jifeng Dai</strong>  <br />
International Conference on Learning Representations (ICLR), 2020. <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1908.08530">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a> <br />
Weijie Su*<sup>+</sup>, Xizhou Zhu*<sup>+</sup>,  Yue Cao, Bin Li, Lewei Lu, Furu Wei, and <strong>Jifeng Dai</strong>  <br />
International Conference on Learning Representations (ICLR), 2020. <br />
<a href="https://github.com/jackroos/VL-BERT">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1904.05873">An Empirical Study of Spatial Attention Mechanisms in Deep Networks</a> <br />
Xizhou Zhu*<sup>+</sup>, Dazhi Cheng*<sup>+</sup>, Zheng Zhang<sup>+</sup>, Steve Lin, and <strong>Jifeng Dai</strong>  <br />
International Conference on Computer Vision (ICCV), 2019. <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1811.11168">Deformable ConvNets v2: More Deformable, Better Results</a> <br />
Xizhou Zhu*, Han Hu, Steve Lin, and <strong>Jifeng Dai</strong>  <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.<br />
<a href="https://github.com/msracver/Deformable-ConvNets">The updated operators utilized in Deformable ConvNets v2 are provided here!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1811.11167">Integrated Object Detection and Tracking with Tracklet-Conditioned Detection</a> <br />
Zheng Zhang<sup>+</sup>, Dazhi Cheng*<sup>+</sup>, Xizhou Zhu*<sup>+</sup>, Steve Lin, and <strong>Jifeng Dai</strong>  <br />
Arxiv Tech Report, 2018.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1804.05830">Towards High Performance Video Object Detection for Mobiles</a> <br />
Xizhou Zhu*, <strong>Jifeng Dai</strong>, Xingchi Zhu*, Yichen Wei, and Lu Yuan  <br />
Arxiv Tech Report, 2018.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1803.07066">Learning Region Features for Object Detection</a> <br />
Jiayuan Gu*, Han Hu, Liwei Wang, Yichen Wei, and <strong>Jifeng Dai</strong>  <br />
European Conference on Computer Vision (ECCV), 2018.<br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1711.11575">Relation Networks for Object Detection</a> <br />
Han Hu<sup>+</sup>, Jiayuan Gu*<sup>+</sup>, Zheng Zhang<sup>+</sup>, <strong>Jifeng Dai</strong>, and Yichen Wei <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. (<strong>Oral</strong>) <br />
<a href="https://github.com/msracver/Relation-Networks-for-Object-Detection">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1711.11577">Towards High Performance Video Object Detection</a> <br />
Xizhou Zhu*, <strong>Jifeng Dai</strong>, Lu Yuan, and Yichen Wei <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. (<strong>Spotlight</strong>) <br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1703.06211">Deformable Convolutional Networks</a> <br />
<strong>Jifeng Dai</strong><sup>+</sup>, Haozhi Qi*<sup>+</sup>, Yuwen Xiong*<sup>+</sup>, Yi Li*<sup>+</sup>, Guodong Zhang*<sup>+</sup>, Han Hu, and Yichen Wei <br />
International Conference on Computer Vision (ICCV), 2017. (<strong>Oral</strong>) <br />
<a href="https://github.com/msracver/Deformable-ConvNets">Code is available!</a> <br />
Slides of <a href="http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf">ICCV Oral</a>, <a href="http://presentations.cocodataset.org/COCO17-Detect-MSRA.pdf">COCO2017 workshop</a><br /></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1703.10025">Flow-Guided Feature Aggregation for Video Object Detection</a> <br />
Xizhou Zhu*, Yujie Wang*, <strong>Jifeng Dai</strong>, Lu Yuan, and Yichen Wei <br />
International Conference on Computer Vision (ICCV), 2017. <br />
<a href="https://github.com/msracver/Flow-Guided-Feature-Aggregation">Code is available!</a> <br />
<a href="https://www.youtube.com/watch?v=R2h3DbTPvVg">Video</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1611.07709">Fully Convolutional Instance-aware Semantic Segmentation</a> <br />
Yi Li*<sup>+</sup>, Haozhi Qi*<sup>+</sup>, <strong>Jifeng Dai</strong>, Xiangyang Ji, and Yichen Wei <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. (<strong>Spotlight</strong>)<br />
1-st place winner of the COCO 2016 segmentation challenge  <br />
0.24 sec/image test speed (using ResNet-101 net) <br />
<a href="https://github.com/msracver/FCIS">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="https://arxiv.org/abs/1611.07715">Deep Feature Flow for Video Recognition</a> <br />
Xizhou Zhu*, Yuwen Xiong*, <strong>Jifeng Dai</strong>, Lu Yuan, and Yichen Wei <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. <br />
<a href="https://github.com/msracver/Deep-Feature-Flow">Code is available!</a> <br />
<a href="https://www.youtube.com/watch?v=J0rMHE6ehGw">Video</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://arxiv.org/abs/1605.06409">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a> <br />
<strong>Jifeng Dai</strong>, Yi Li, Kaiming He, and Jian Sun <br />
Neural Information Processing Systems (NIPS), 2016. <br />
<a href="https://github.com/daijifeng001/R-FCN">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://arxiv.org/abs/1603.08678">Instance-sensitive Fully Convolutional Networks</a> <br />
<strong>Jifeng Dai</strong>, Kaiming He, Yi Li, Shaoqing Ren, and Jian Sun <br />
European Conference on Computer Vision (ECCV), 2016.</p>
</blockquote>

<!-- -->

<blockquote>
<p><a href="http://arxiv.org/abs/1604.05144">ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation</a> <br />
Di Lin*, <strong>Jifeng Dai</strong>, Jiaya Jia, Kaiming He, and Jian Sun <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. (<strong>Oral</strong>) <br />
<a href="http://www.jifengdai.org/downloads/scribble_sup/">PASCAL-scribble dataset is available here</a>!</p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://arxiv.org/abs/1512.04412">Instance-aware Semantic Segmentation via Multi-task Network Cascades</a> <br />
<strong>Jifeng Dai</strong>, Kaiming He, and Jian Sun <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. (<strong>Oral</strong>) <br />
1-st place winner of the COCO 2015 segmentation challenge  <br />
0.36 sec/image test speed (using VGG-16 net) <br />
<a href="https://github.com/daijifeng001/MNC">Code is available!</a></p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://arxiv.org/abs/1503.01640">BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</a> <br />
<strong>Jifeng Dai</strong>, Kaiming He, and Jian Sun <br />
International Conference on Computer Vision (ICCV), 2015.</p>
</blockquote>

<!-- -->

<blockquote>
<p><a href="http://arxiv.org/abs/1412.1283">Convolutional Feature Masking for Joint Object and Stuff Segmentation</a> <br />
<strong>Jifeng Dai</strong>, Kaiming He, and Jian Sun <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.</p>
</blockquote>

<!-- -->  

<blockquote>
<p><a href="http://arxiv.org/abs/1412.6296">Generative Modeling of Convolutional Neural Networks</a> <br />
<strong>Jifeng Dai</strong>, Yang Lu, and Ying-Nian Wu <br />
International Conference on Learning Representations (ICLR), 2015. <br />
<a href="http://www.stat.ucla.edu/%7Eyang.lu/Project/generativeCNN/main.html">project page</a></p>
</blockquote>

<!-- -->   

<blockquote>
<p><a href="http://www.jifengdai.org/publications/CVPR_2014.pdf">Unsupervised Learning of Dictionaries of Hierarchical Compositional Models</a> <br />
<strong>Jifeng Dai</strong>, Yi Hong, Wenze Hu, Song-Chun Zhu, and Ying-Nian Wu <br />
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014. <br />
<a href="http://www.stat.ucla.edu/%7Ejifeng.dai/research/HCM.html">project page</a>  </p>
</blockquote>

<!-- -->  

<blockquote>
<p><a href="http://www.jifengdai.org/publications/ICCV_Cosegmentation_2013.pdf">Cosegmentation and Cosketch by Unsupervised Learning</a> <br />
<strong>Jifeng Dai</strong>, Ying-Nian Wu, Jie Zhou, and Song-Chun Zhu <br />
International Conference on Computer Vision (ICCV), 2013. <br />
<a href="http://www.stat.ucla.edu/%7Ejifeng.dai/research/CosegmentationCosketch.html">project page</a> </p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://www.jifengdai.org/publications/ICPR_Mining_2012.pdf">Mining Sub-categories For Object Detection</a> <br />
<strong>Jifeng Dai</strong>, Jianjiang Feng, and Jie Zhou <br />
International Conference on Pattern Recognition (ICPR), 2012. (<strong>Oral</strong>)</p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://www.jifengdai.org/publications/Newsletter_Ridge_2013.pdf">Ridge Based Palmprint Matching</a> <br />
<strong>Jifeng Dai</strong>, Jianjiang Feng, and Jie Zhou <br />
IEEE Biometrics Council Newsletter, 7:4-5, 2013. (Invited paper)  </p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://www.jifengdai.org/publications/PAMI_Robust_2012.pdf">Robust and Efficient Ridge-Based Palmprint Matching</a> <br />
<strong>Jifeng Dai</strong>, Jianjiang Feng, and Jie Zhou  <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 34(8):1618–1632, 2012.</p>
</blockquote>

<!-- --> 

<blockquote>
<p><a href="http://www.jifengdai.org/publications/PAMI_MultiFeature_2011.pdf">Multi-feature Based High-resolution Palmprint Recognition</a> <br />
<strong>Jifeng Dai</strong>, and Jie Zhou <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 33(5):945–957, 2011.</p>
</blockquote>

<h2>Students &amp; Interns</h2>

<h3>Current</h3>

<ul>
<li><a href="https://scholar.google.com/citations?user=ECDe6IIAAAAJ&amp;hl=en">Weijie Su</a></li>
<li><a href="https://scholar.google.com/citations?user=sXHFIBkAAAAJ&amp;hl=zh-CN">Chenxin Tao</a></li>
<li><a href="https://scholar.google.com/citations?user=qHqQsY4AAAAJ&amp;hl=zh-CN">Hao Li</a></li>
<li><a href="https://dblp.org/pid/244/7280.html">JIngguo Zhu</a></li>
<li>Changyao Tian</li>
<li>Zhaokai Wang</li>
<li>Chenyu Yang</li>
<li>Muyan Zhong</li>
<li>Sen Xing</li>
<li>Zhangwei Gao</li>
<li>Yuchen Duan</li>
</ul>

<h3>Former</h3>

<p>(Sorry if I missed someone.)</p>

<ul>
<li><a href="https://scholar.google.com.hk/citations?user=MW36lZUAAAAJ&amp;hl=en">Yi Li</a></li>
<li><a href="https://scholar.google.com.hk/citations?user=iyVHKkcAAAAJ&amp;hl=en">Haozhi Qi</a></li>
<li><a href="https://scholar.google.com/citations?user=02RXI00AAAAJ&amp;hl=en">Xizhou Zhu</a></li>
<li><a href="https://scholar.google.com.hk/citations?user=7YALCcIAAAAJ&amp;hl=en">Yuwen Xiong</a></li>
<li><a href="https://scholar.google.com.hk/citations?user=rW0r-hMAAAAJ&amp;hl=en">Di Lin</a></li>
<li><a href="https://scholar.google.com/citations?user=YsXTcgYAAAAJ&amp;hl=en">Jiayuan Gu</a></li>
<li><a href="https://scholar.google.com/citations?user=JETJjHoAAAAJ&amp;hl=en">Bowen Cheng</a></li>
<li><a href="https://scholar.google.com/citations?user=xzr7na8AAAAJ&amp;hl=en">Dazhi Cheng</a></li>
<li><a href="https://scholar.google.com/citations?user=cwXx4EQAAAAJ&amp;hl=en">Hang Gao</a></li>
<li><a href="https://www.taokong.org/">Tao Kong</a></li>
<li><a href="https://justimyhxu.github.io/">Yinghao Xu</a></li>
<li><a href="https://zdaxie.github.io/">Zhenda Xie</a></li>
<li><a href="https://zeliu98.github.io/">Ze Liu</a></li>
<li><a href="https://github.com/tgxs002">Xiaoshi Wu</a></li>
<li><a href="https://scholar.google.com/citations?user=f6Wu6MMAAAAJ&amp;hl=fr">Jingqiu Zhou</a></li>
<li><a href="https://scholar.google.com/citations?user=R3Av3IkAAAAJ&amp;hl=en">Haiyang Wang</a></li>
<li><a href="https://scholar.google.com/citations?user=iLOoUqIAAAAJ&amp;hl=en">Yuntao Chen</a></li>
<li><a href="https://github.com/tianwen-fu">Tianwen Fu</a></li>
<li>Hao Tian</li>
<li>Yue Cao</li>
<li><a href="https://kyleleey.github.io/">Zizhang Li</a></li>
<li><a href="https://scholar.google.com.hk/citations?user=H2fJLqEAAAAJ&amp;hl=en">Zhiqi Li</a></li>
</ul>

<h2>Competitions &amp; Awards</h2>

<ul>
<li>CVPR 2023 Best Paper Award</li>
<li>1st place in Waymo 2022 3D Camera-Only Detection Task</li>
<li>3rd place in COCO object detection challenge 2017</li>
<li>1st place in COCO instance segmentation challenge 2016</li>
<li>1st place in COCO object detection &amp; instance segmentation challenge 2015</li>
</ul>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82196129-1', 'auto');
  ga('send', 'pageview');

</script>

</body>
</html>